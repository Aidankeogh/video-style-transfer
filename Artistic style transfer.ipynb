{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.0\n",
      "0.2.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import cv2\n",
    "from neural_style import stylize_img\n",
    "from scipy.misc import imsave\n",
    "import cvbase as cvb\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory with video frames and flo files. (Created from setup.ipynb)\n",
    "videoDir = \"test-videos/iguana_pan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all image/flo names\n",
    "images = sorted(glob.glob( videoDir + \"/frame*.png\"))\n",
    "flos = sorted(glob.glob(videoDir + \"/*.flo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load images as tensors, flow warping can be done on CPU because it isn't computationally intensive\n",
    "\n",
    "device = \"cpu\"\n",
    "imsize = (480, 640)\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),# scale imported image\n",
    "    transforms.ToTensor()])  # transform it into a torch tensor\n",
    "unloader = transforms.ToPILImage()\n",
    "\n",
    "def im_loader(image_name, flipped = False):\n",
    "    image = Image.open(image_name)\n",
    "    \n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Warps an input image by the optical flow\n",
    "\n",
    "def flow_warp(original, flostr):\n",
    "    original = (original.view(3,480,640).numpy()).clip(0,1)\n",
    "    original = np.swapaxes(original,2,0)\n",
    "    original = np.swapaxes(original,1,0)\n",
    "    \n",
    "    flo = cvb.read_flow(flostr)\n",
    "    flo = np.swapaxes(flo,2,0)\n",
    "    flo = np.swapaxes(flo,1,2)\n",
    "    _, h, w = flo.shape\n",
    "    flow_map = np.zeros(flo.shape,dtype=np.float32)\n",
    "    \n",
    "    for y in range(h):\n",
    "        flow_map[1,y,:] = float(y) - flo[1,y,:]\n",
    "    for x in range(w):\n",
    "        flow_map[0,:,x] = float(x) - flo[0,:,x]\n",
    "    warped = np.zeros_like(original)\n",
    "    \n",
    "    warped[:,:,0] = cv2.remap(original[:,:,0],flow_map[0],flow_map[1],\n",
    "                   interpolation=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    warped[:,:,1] = cv2.remap(original[:,:,1],flow_map[0],flow_map[1],\n",
    "                   interpolation=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    warped[:,:,2] = cv2.remap(original[:,:,2],flow_map[0],flow_map[1],\n",
    "                   interpolation=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    \n",
    "    return warped.clip(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads im and flo file into warp function\n",
    "\n",
    "def get_warped(im1,flo):\n",
    "    flo_im = cvb.flow2rgb(cvb.read_flow(flo))\n",
    "    im1 = im_loader(im1)\n",
    "    warped = flow_warp(im1,flo)\n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the tag and style image to what you like.\n",
    "style_img = 'style/kazao.jpg'\n",
    "style_tag = 'kazaotest'\n",
    "def style_name(img_name):\n",
    "    img_name = img_name.split(\"frame\")\n",
    "    return img_name[-2] + style_tag + img_name[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the style transfer model..\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 0.000034 Content Loss: 44.684731\n",
      "()\n",
      "run [100]:\n",
      "Style Loss : 0.000009 Content Loss: 40.283005\n",
      "()\n",
      "run [150]:\n",
      "Style Loss : 0.000005 Content Loss: 36.515656\n",
      "()\n",
      "run [200]:\n",
      "Style Loss : 0.000004 Content Loss: 34.560608\n",
      "()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-667b9e8a7766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwarp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'temp.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstylize_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstyle_img\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstyle_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m700\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwarped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_warped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyle_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mflos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimsave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aidan/cse190-video-style-transfer/neural_style.pyc\u001b[0m in \u001b[0;36mstylize_img\u001b[0;34m(content_img, style_img, input_img, output_img, num_steps, style_weight, content_weight)\u001b[0m\n\u001b[1;32m    488\u001b[0m     output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n\u001b[1;32m    489\u001b[0m                                 \u001b[0mcontent_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstyle_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstyle_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m                                 content_weight=content_weight)\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/aidan/cse190-video-style-transfer/neural_style.pyc\u001b[0m in \u001b[0;36mrun_style_transfer\u001b[0;34m(cnn, normalization_mean, normalization_std, content_img, style_img, input_img, num_steps, style_weight, content_weight)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstyle_score\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcontent_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;31m# a last correction...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/optim/lbfgs.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    178\u001b[0m                 \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH_diag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mbe_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m                     \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbe_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Saves the frame warped by the optical flow in temp.png (Temp because it's continually overwritten)\n",
    "warp = 'temp.png'\n",
    "\n",
    "#Run the stylization algorithm for many steps on the first image\n",
    "stylize_img(images[0],style_img,images[0],style_name(images[0]), num_steps = 700)\n",
    "warped = get_warped(style_name(images[0]),flos[0])\n",
    "imsave(warp, warped)\n",
    "\n",
    "#For every image after, it only needs to be ran for a fraction of the iterations\n",
    "for im, flo in zip(images[1:],flos[1:]):\n",
    "    stylize_img(im, style_img, warp, style_name(im), num_steps = 200)\n",
    "    warped = get_warped(style_name(im),flo)\n",
    "    imsave(\"temp.png\", warped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
